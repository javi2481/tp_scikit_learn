# üß© 06 - Pipelines y Preprocesamiento de Modelos de Machine Learning

## 1Ô∏è‚É£ Introducci√≥n üöÄ

El proceso de Machine Learning (ML) no se limita al entrenamiento de un modelo: implica una serie de pasos bien organizados que transforman datos crudos en predicciones √∫tiles.  
Uno de los enfoques m√°s potentes para manejar esta complejidad es el uso de **pipelines**, que permiten automatizar, reproducir y versionar el flujo completo.

> üéØ Un buen pipeline garantiza que las transformaciones aplicadas durante el desarrollo tambi√©n se apliquen de forma id√©ntica en producci√≥n.

---

## 2Ô∏è‚É£ ¬øQu√© es un Pipeline de Machine Learning? üõ†Ô∏è

Un **pipeline de ML** es una secuencia automatizada de pasos que organiza el flujo de trabajo completo:

### üîÑ Etapas t√≠picas:

1. **üì• Ingesta de datos**
   - Carga desde archivos, APIs, bases de datos.
   - Control de versiones con herramientas como DVC.

2. **üßπ Preprocesamiento**
   - Limpieza, imputaci√≥n, escalado, codificaci√≥n, ingenier√≠a de variables.

3. **‚úÇÔ∏è Divisi√≥n del dataset**
   - Separaci√≥n en entrenamiento, validaci√≥n y test.

4. **ü§ñ Entrenamiento del modelo**
   - Elecci√≥n del algoritmo.
   - Ajuste de hiperpar√°metros (GridSearch, RandomizedSearch).

5. **üìä Evaluaci√≥n**
   - Validaci√≥n cruzada, m√©tricas y visualizaciones.
   - Interpretabilidad con SHAP, LIME.

6. **üöÄ Despliegue**
   - Serializaci√≥n con `joblib`, `pickle`, `ONNX`.
   - Exposici√≥n como API, procesos batch, o dispositivos edge.

7. **üìà Monitoreo**
   - Seguimiento de m√©tricas en producci√≥n.
   - Detecci√≥n de _data drift_ y _model drift_.

---

## üîß Herramientas populares en producci√≥n

- **Gesti√≥n de pipelines**: `Scikit-learn`, `MLflow`, `Kubeflow`, `Airflow`  
- **Versionado**: `DVC`, `MLflow`  
- **Automatizaci√≥n**: `Jenkins`, `GitHub Actions`  
- **Tracking de experimentos**: `Weights & Biases`, `MLflow`

---

## 3Ô∏è‚É£ Etapas del Preprocesamiento de Datos üßº

Un buen preprocesamiento mejora la calidad de los datos y el rendimiento del modelo.

### Etapas comunes:

1. **üßΩ Limpieza de datos**  
   - Quitar duplicados, corregir errores, detectar valores at√≠picos.

2. **üß© Imputaci√≥n de valores faltantes**  
   - Usar media, mediana o modelos predictivos.

3. **üìè Escalado de variables num√©ricas**  
   - `StandardScaler`: media 0, desv√≠o est√°ndar 1  
   - `MinMaxScaler`: escala los valores entre 0 y 1

4. **üè∑Ô∏è Codificaci√≥n de variables categ√≥ricas**  
   - `OneHotEncoder`, `LabelEncoder`, o embeddings

5. **üß† Feature Engineering**  
   - Crear nuevas variables √∫tiles

6. **‚úÇÔ∏è Selecci√≥n de variables**  
   - Usar m√©todos como `SelectKBest`, `RFE`, o an√°lisis de correlaci√≥n

> ‚ö†Ô∏è Un preprocesamiento deficiente puede introducir sesgos o afectar la precisi√≥n del modelo.

---

## 4Ô∏è‚É£ Buenas Pr√°cticas üß†

‚úÖ Automatizar todo dentro del pipeline  
‚úÖ Aplicar las mismas transformaciones en desarrollo y producci√≥n  
‚úÖ Evitar *data leakage* (usar datos del test sin querer)  
‚úÖ Separar claramente preprocesamiento y modelado  
‚úÖ Serializar el pipeline completo  
‚úÖ Versionar el modelo y los transformadores

---

## 5Ô∏è‚É£ Ejemplo pr√°ctico con Scikit-learn ü§ñ

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# üå∏ 1. Cargar dataset Iris
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# ‚úÇÔ∏è 2. Dividir en train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# üßπ 3. Preprocesamiento num√©rico
numeric_features = X.columns.tolist()
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),       # Imputar valores faltantes con la media
    ('scaler', StandardScaler())                       # Escalar los datos num√©ricos
])

# üîÑ 4. Armar ColumnTransformer (solo num√©rico en este caso)
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, numeric_features)
])

# üèóÔ∏è 5. Crear pipeline completo
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),                    # Primer paso: preprocesar
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Segundo paso: modelo
])

# üß† 6. Entrenar modelo
pipeline.fit(X_train, y_train)

# üìä 7. Predecir y evaluar
y_pred = pipeline.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.3f}")
```

---

## 6Ô∏è‚É£ Conclusi√≥n üéì

Los pipelines permiten construir flujos de trabajo robustos, reutilizables y adaptables tanto en desarrollo como en producci√≥n.

> ‚úÖ Automatizaci√≥n + reproducibilidad + buenas pr√°cticas = proyectos de ML profesionales y sostenibles.
